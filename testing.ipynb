{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_auto_class', '_backward_compatibility_gradient_checkpointing', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_convert_head_mask_to_5d', '_create_repo', '_expand_inputs_for_generation', '_extract_past_from_model_output', '_forward_hooks', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_from_config', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_decoder_start_token_id', '_get_files_timestamps', '_get_logits_processor', '_get_logits_warper', '_get_name', '_get_resized_embeddings', '_get_resized_lm_head', '_get_stopping_criteria', '_hook_rss_memory_post_forward', '_hook_rss_memory_pre_forward', '_init_weights', '_initialize_weights', '_is_full_backward_hook', '_is_hf_initialized', '_keep_in_fp32_modules', '_keys_to_ignore_on_load_missing', '_keys_to_ignore_on_load_unexpected', '_keys_to_ignore_on_save', '_load_from_state_dict', '_load_pretrained_model', '_load_pretrained_model_low_mem', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_initialize_input_ids_for_generation', '_maybe_warn_non_full_backward_hook', '_merge_criteria_processor_list', '_modules', '_named_members', '_no_split_modules', '_non_persistent_buffers_set', '_parameters', '_prepare_attention_mask_for_generation', '_prepare_decoder_input_ids_for_generation', '_prepare_encoder_decoder_kwargs_for_generation', '_prepare_model_inputs', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_reorder_cache', '_replicate_for_data_parallel', '_resize_token_embeddings', '_save_to_state_dict', '_set_default_torch_dtype', '_set_gradient_checkpointing', '_skip_keys_device_placement', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_tie_encoder_decoder_weights', '_tie_or_clone_weights', '_update_model_kwargs_for_generation', '_upload_modified_files', '_validate_model_class', '_validate_model_kwargs', '_version', 'add_memory_hooks', 'add_module', 'adjust_logits_during_generation', 'apply', 'assisted_decoding', 'base_model', 'base_model_prefix', 'beam_sample', 'beam_search', 'bfloat16', 'buffers', 'call_super_init', 'can_generate', 'children', 'compute_transition_scores', 'config', 'config_class', 'constrained_beam_search', 'contrastive_search', 'cpu', 'create_extended_attention_mask_for_decoder', 'cuda', 'deparallelize', 'device', 'device_map', 'disable_input_require_grads', 'double', 'dtype', 'dummy_inputs', 'dump_patches', 'enable_input_require_grads', 'estimate_tokens', 'eval', 'extra_repr', 'float', 'floating_point_ops', 'forward', 'framework', 'from_pretrained', 'generate', 'generation_config', 'get_buffer', 'get_extended_attention_mask', 'get_extra_state', 'get_head_mask', 'get_input_embeddings', 'get_memory_footprint', 'get_output_embeddings', 'get_parameter', 'get_position_embeddings', 'get_submodule', 'gradient_checkpointing_disable', 'gradient_checkpointing_enable', 'greedy_search', 'group_beam_search', 'half', 'init_weights', 'invert_attention_mask', 'ipu', 'is_gradient_checkpointing', 'is_loaded_in_4bit', 'is_loaded_in_8bit', 'is_parallelizable', 'is_quantized', 'lm_head', 'load_state_dict', 'main_input_name', 'model_parallel', 'modules', 'name_or_path', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'num_parameters', 'parallelize', 'parameters', 'post_init', 'prepare_inputs_for_generation', 'prune_heads', 'push_to_hub', 'register_backward_hook', 'register_buffer', 'register_for_auto_class', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'reset_memory_hooks_state', 'resize_position_embeddings', 'resize_token_embeddings', 'retrieve_modules_from_names', 'reverse_bettertransformer', 'sample', 'save_pretrained', 'set_extra_state', 'set_input_embeddings', 'set_output_embeddings', 'share_memory', 'state_dict', 'supports_gradient_checkpointing', 'tie_weights', 'to', 'to_bettertransformer', 'to_empty', 'train', 'training', 'transformer', 'type', 'warnings_issued', 'xpu', 'zero_grad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.Tensor([torch.tensor([1]),torch.tensor([1]),torch.tensor([1])]).unsqueeze(0)\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_destination\n",
      "__annotations__\n",
      "__call__\n",
      "__class__\n",
      "__delattr__\n",
      "__dict__\n",
      "__dir__\n",
      "__doc__\n",
      "__eq__\n",
      "__format__\n",
      "__ge__\n",
      "__getattr__\n",
      "__getattribute__\n",
      "__gt__\n",
      "__hash__\n",
      "__init__\n",
      "__init_subclass__\n",
      "__le__\n",
      "__lt__\n",
      "__module__\n",
      "__ne__\n",
      "__new__\n",
      "__reduce__\n",
      "__reduce_ex__\n",
      "__repr__\n",
      "__setattr__\n",
      "__setstate__\n",
      "__sizeof__\n",
      "__str__\n",
      "__subclasshook__\n",
      "__weakref__\n",
      "_apply\n",
      "_auto_class\n",
      "_backward_compatibility_gradient_checkpointing\n",
      "_backward_hooks\n",
      "_backward_pre_hooks\n",
      "_buffers\n",
      "_call_impl\n",
      "_convert_head_mask_to_5d\n",
      "_create_repo\n",
      "_expand_inputs_for_generation\n",
      "_extract_past_from_model_output\n",
      "_forward_hooks\n",
      "_forward_hooks_with_kwargs\n",
      "_forward_pre_hooks\n",
      "_forward_pre_hooks_with_kwargs\n",
      "_from_config\n",
      "_get_backward_hooks\n",
      "_get_backward_pre_hooks\n",
      "_get_decoder_start_token_id\n",
      "_get_files_timestamps\n",
      "_get_logits_processor\n",
      "_get_logits_warper\n",
      "_get_name\n",
      "_get_resized_embeddings\n",
      "_get_resized_lm_head\n",
      "_get_stopping_criteria\n",
      "_hook_rss_memory_post_forward\n",
      "_hook_rss_memory_pre_forward\n",
      "_init_weights\n",
      "_initialize_weights\n",
      "_is_full_backward_hook\n",
      "_is_hf_initialized\n",
      "_keep_in_fp32_modules\n",
      "_keys_to_ignore_on_load_missing\n",
      "_keys_to_ignore_on_load_unexpected\n",
      "_keys_to_ignore_on_save\n",
      "_load_from_state_dict\n",
      "_load_pretrained_model\n",
      "_load_pretrained_model_low_mem\n",
      "_load_state_dict_post_hooks\n",
      "_load_state_dict_pre_hooks\n",
      "_maybe_initialize_input_ids_for_generation\n",
      "_maybe_warn_non_full_backward_hook\n",
      "_merge_criteria_processor_list\n",
      "_modules\n",
      "_named_members\n",
      "_no_split_modules\n",
      "_non_persistent_buffers_set\n",
      "_parameters\n",
      "_prepare_attention_mask_for_generation\n",
      "_prepare_decoder_input_ids_for_generation\n",
      "_prepare_encoder_decoder_kwargs_for_generation\n",
      "_prepare_model_inputs\n",
      "_register_load_state_dict_pre_hook\n",
      "_register_state_dict_hook\n",
      "_reorder_cache\n",
      "_replicate_for_data_parallel\n",
      "_resize_token_embeddings\n",
      "_save_to_state_dict\n",
      "_set_default_torch_dtype\n",
      "_set_gradient_checkpointing\n",
      "_skip_keys_device_placement\n",
      "_slow_forward\n",
      "_state_dict_hooks\n",
      "_state_dict_pre_hooks\n",
      "_tie_encoder_decoder_weights\n",
      "_tie_or_clone_weights\n",
      "_update_model_kwargs_for_generation\n",
      "_upload_modified_files\n",
      "_validate_model_class\n",
      "_validate_model_kwargs\n",
      "_version\n",
      "add_memory_hooks\n",
      "add_module\n",
      "adjust_logits_during_generation\n",
      "apply\n",
      "assisted_decoding\n",
      "base_model\n",
      "base_model_prefix\n",
      "beam_sample\n",
      "beam_search\n",
      "bfloat16\n",
      "buffers\n",
      "call_super_init\n",
      "can_generate\n",
      "children\n",
      "compute_transition_scores\n",
      "config\n",
      "config_class\n",
      "constrained_beam_search\n",
      "contrastive_search\n",
      "cpu\n",
      "create_extended_attention_mask_for_decoder\n",
      "cuda\n",
      "deparallelize\n",
      "device\n",
      "device_map\n",
      "disable_input_require_grads\n",
      "double\n",
      "dtype\n",
      "dummy_inputs\n",
      "dump_patches\n",
      "enable_input_require_grads\n",
      "estimate_tokens\n",
      "eval\n",
      "extra_repr\n",
      "float\n",
      "floating_point_ops\n",
      "forward\n",
      "framework\n",
      "from_pretrained\n",
      "generate\n",
      "generation_config\n",
      "get_buffer\n",
      "get_extended_attention_mask\n",
      "get_extra_state\n",
      "get_head_mask\n",
      "get_input_embeddings\n",
      "get_memory_footprint\n",
      "get_output_embeddings\n",
      "get_parameter\n",
      "get_position_embeddings\n",
      "get_submodule\n",
      "gradient_checkpointing_disable\n",
      "gradient_checkpointing_enable\n",
      "greedy_search\n",
      "group_beam_search\n",
      "half\n",
      "init_weights\n",
      "invert_attention_mask\n",
      "ipu\n",
      "is_gradient_checkpointing\n",
      "is_loaded_in_4bit\n",
      "is_loaded_in_8bit\n",
      "is_parallelizable\n",
      "is_quantized\n",
      "lm_head\n",
      "load_state_dict\n",
      "main_input_name\n",
      "model_parallel\n",
      "modules\n",
      "name_or_path\n",
      "named_buffers\n",
      "named_children\n",
      "named_modules\n",
      "named_parameters\n",
      "num_parameters\n",
      "parallelize\n",
      "parameters\n",
      "post_init\n",
      "prepare_inputs_for_generation\n",
      "prune_heads\n",
      "push_to_hub\n",
      "register_backward_hook\n",
      "register_buffer\n",
      "register_for_auto_class\n",
      "register_forward_hook\n",
      "register_forward_pre_hook\n",
      "register_full_backward_hook\n",
      "register_full_backward_pre_hook\n",
      "register_load_state_dict_post_hook\n",
      "register_module\n",
      "register_parameter\n",
      "register_state_dict_pre_hook\n",
      "requires_grad_\n",
      "reset_memory_hooks_state\n",
      "resize_position_embeddings\n",
      "resize_token_embeddings\n",
      "retrieve_modules_from_names\n",
      "reverse_bettertransformer\n",
      "sample\n",
      "save_pretrained\n",
      "set_extra_state\n",
      "set_input_embeddings\n",
      "set_output_embeddings\n",
      "share_memory\n",
      "state_dict\n",
      "supports_gradient_checkpointing\n",
      "tie_weights\n",
      "to\n",
      "to_bettertransformer\n",
      "to_empty\n",
      "train\n",
      "training\n",
      "transformer\n",
      "type\n",
      "warnings_issued\n",
      "xpu\n",
      "zero_grad\n"
     ]
    }
   ],
   "source": [
    "for i in a:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through a and check if any string contrains \"layer\"\n",
    "\n",
    "for i in a:\n",
    "    if \"layer\" in i:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.34                 a\n"
     ]
    }
   ],
   "source": [
    "x = 1.345\n",
    "\n",
    "print(f\"{x:<20.2f} a\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPTJ_LAYERS = ['transformer.wte.weight', 'transformer.h.0.ln_1.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.0.attn.k_proj.weight', 'transformer.h.0.attn.v_proj.weight', 'transformer.h.0.attn.q_proj.weight', 'transformer.h.0.attn.out_proj.weight', 'transformer.h.0.mlp.fc_in.weight', 'transformer.h.0.mlp.fc_in.bias', 'transformer.h.0.mlp.fc_out.weight', 'transformer.h.0.mlp.fc_out.bias', 'transformer.h.1.ln_1.weight', 'transformer.h.1.ln_1.bias', 'transformer.h.1.attn.k_proj.weight', 'transformer.h.1.attn.v_proj.weight', 'transformer.h.1.attn.q_proj.weight', 'transformer.h.1.attn.out_proj.weight', 'transformer.h.1.mlp.fc_in.weight', 'transformer.h.1.mlp.fc_in.bias', 'transformer.h.1.mlp.fc_out.weight', 'transformer.h.1.mlp.fc_out.bias', 'transformer.h.2.ln_1.weight', 'transformer.h.2.ln_1.bias', 'transformer.h.2.attn.k_proj.weight', 'transformer.h.2.attn.v_proj.weight', 'transformer.h.2.attn.q_proj.weight', 'transformer.h.2.attn.out_proj.weight', 'transformer.h.2.mlp.fc_in.weight', 'transformer.h.2.mlp.fc_in.bias', 'transformer.h.2.mlp.fc_out.weight', 'transformer.h.2.mlp.fc_out.bias', 'transformer.h.3.ln_1.weight', 'transformer.h.3.ln_1.bias', 'transformer.h.3.attn.k_proj.weight', 'transformer.h.3.attn.v_proj.weight', 'transformer.h.3.attn.q_proj.weight', 'transformer.h.3.attn.out_proj.weight', 'transformer.h.3.mlp.fc_in.weight', 'transformer.h.3.mlp.fc_in.bias', 'transformer.h.3.mlp.fc_out.weight', 'transformer.h.3.mlp.fc_out.bias', 'transformer.h.4.ln_1.weight', 'transformer.h.4.ln_1.bias', 'transformer.h.4.attn.k_proj.weight', 'transformer.h.4.attn.v_proj.weight', 'transformer.h.4.attn.q_proj.weight', 'transformer.h.4.attn.out_proj.weight', 'transformer.h.4.mlp.fc_in.weight', 'transformer.h.4.mlp.fc_in.bias', 'transformer.h.4.mlp.fc_out.weight', 'transformer.h.4.mlp.fc_out.bias', 'transformer.h.5.ln_1.weight', 'transformer.h.5.ln_1.bias', 'transformer.h.5.attn.k_proj.weight', 'transformer.h.5.attn.v_proj.weight', 'transformer.h.5.attn.q_proj.weight', 'transformer.h.5.attn.out_proj.weight', 'transformer.h.5.mlp.fc_in.weight', 'transformer.h.5.mlp.fc_in.bias', 'transformer.h.5.mlp.fc_out.weight', 'transformer.h.5.mlp.fc_out.bias', 'transformer.h.6.ln_1.weight', 'transformer.h.6.ln_1.bias', 'transformer.h.6.attn.k_proj.weight', 'transformer.h.6.attn.v_proj.weight', 'transformer.h.6.attn.q_proj.weight', 'transformer.h.6.attn.out_proj.weight', 'transformer.h.6.mlp.fc_in.weight', 'transformer.h.6.mlp.fc_in.bias', 'transformer.h.6.mlp.fc_out.weight', 'transformer.h.6.mlp.fc_out.bias', 'transformer.h.7.ln_1.weight', 'transformer.h.7.ln_1.bias', 'transformer.h.7.attn.k_proj.weight', 'transformer.h.7.attn.v_proj.weight', 'transformer.h.7.attn.q_proj.weight', 'transformer.h.7.attn.out_proj.weight', 'transformer.h.7.mlp.fc_in.weight', 'transformer.h.7.mlp.fc_in.bias', 'transformer.h.7.mlp.fc_out.weight', 'transformer.h.7.mlp.fc_out.bias', 'transformer.h.8.ln_1.weight', 'transformer.h.8.ln_1.bias', 'transformer.h.8.attn.k_proj.weight', 'transformer.h.8.attn.v_proj.weight', 'transformer.h.8.attn.q_proj.weight', 'transformer.h.8.attn.out_proj.weight', 'transformer.h.8.mlp.fc_in.weight', 'transformer.h.8.mlp.fc_in.bias', 'transformer.h.8.mlp.fc_out.weight', 'transformer.h.8.mlp.fc_out.bias', 'transformer.h.9.ln_1.weight', 'transformer.h.9.ln_1.bias', 'transformer.h.9.attn.k_proj.weight', 'transformer.h.9.attn.v_proj.weight', 'transformer.h.9.attn.q_proj.weight', 'transformer.h.9.attn.out_proj.weight', 'transformer.h.9.mlp.fc_in.weight', 'transformer.h.9.mlp.fc_in.bias', 'transformer.h.9.mlp.fc_out.weight', 'transformer.h.9.mlp.fc_out.bias', 'transformer.h.10.ln_1.weight', 'transformer.h.10.ln_1.bias', 'transformer.h.10.attn.k_proj.weight', 'transformer.h.10.attn.v_proj.weight', 'transformer.h.10.attn.q_proj.weight', 'transformer.h.10.attn.out_proj.weight', 'transformer.h.10.mlp.fc_in.weight', 'transformer.h.10.mlp.fc_in.bias', 'transformer.h.10.mlp.fc_out.weight', 'transformer.h.10.mlp.fc_out.bias', 'transformer.h.11.ln_1.weight', 'transformer.h.11.ln_1.bias', 'transformer.h.11.attn.k_proj.weight', 'transformer.h.11.attn.v_proj.weight', 'transformer.h.11.attn.q_proj.weight', 'transformer.h.11.attn.out_proj.weight', 'transformer.h.11.mlp.fc_in.weight', 'transformer.h.11.mlp.fc_in.bias', 'transformer.h.11.mlp.fc_out.weight', 'transformer.h.11.mlp.fc_out.bias', 'transformer.h.12.ln_1.weight', 'transformer.h.12.ln_1.bias', 'transformer.h.12.attn.k_proj.weight', 'transformer.h.12.attn.v_proj.weight', 'transformer.h.12.attn.q_proj.weight', 'transformer.h.12.attn.out_proj.weight', 'transformer.h.12.mlp.fc_in.weight', 'transformer.h.12.mlp.fc_in.bias', 'transformer.h.12.mlp.fc_out.weight', 'transformer.h.12.mlp.fc_out.bias', 'transformer.h.13.ln_1.weight', 'transformer.h.13.ln_1.bias', 'transformer.h.13.attn.k_proj.weight', 'transformer.h.13.attn.v_proj.weight', 'transformer.h.13.attn.q_proj.weight', 'transformer.h.13.attn.out_proj.weight', 'transformer.h.13.mlp.fc_in.weight', 'transformer.h.13.mlp.fc_in.bias', 'transformer.h.13.mlp.fc_out.weight', 'transformer.h.13.mlp.fc_out.bias', 'transformer.h.14.ln_1.weight', 'transformer.h.14.ln_1.bias', 'transformer.h.14.attn.k_proj.weight', 'transformer.h.14.attn.v_proj.weight', 'transformer.h.14.attn.q_proj.weight', 'transformer.h.14.attn.out_proj.weight', 'transformer.h.14.mlp.fc_in.weight', 'transformer.h.14.mlp.fc_in.bias', 'transformer.h.14.mlp.fc_out.weight', 'transformer.h.14.mlp.fc_out.bias', 'transformer.h.15.ln_1.weight', 'transformer.h.15.ln_1.bias', 'transformer.h.15.attn.k_proj.weight', 'transformer.h.15.attn.v_proj.weight', 'transformer.h.15.attn.q_proj.weight', 'transformer.h.15.attn.out_proj.weight', 'transformer.h.15.mlp.fc_in.weight', 'transformer.h.15.mlp.fc_in.bias', 'transformer.h.15.mlp.fc_out.weight', 'transformer.h.15.mlp.fc_out.bias', 'transformer.h.16.ln_1.weight', 'transformer.h.16.ln_1.bias', 'transformer.h.16.attn.k_proj.weight', 'transformer.h.16.attn.v_proj.weight', 'transformer.h.16.attn.q_proj.weight', 'transformer.h.16.attn.out_proj.weight', 'transformer.h.16.mlp.fc_in.weight', 'transformer.h.16.mlp.fc_in.bias', 'transformer.h.16.mlp.fc_out.weight', 'transformer.h.16.mlp.fc_out.bias', 'transformer.h.17.ln_1.weight', 'transformer.h.17.ln_1.bias', 'transformer.h.17.attn.k_proj.weight', 'transformer.h.17.attn.v_proj.weight', 'transformer.h.17.attn.q_proj.weight', 'transformer.h.17.attn.out_proj.weight', 'transformer.h.17.mlp.fc_in.weight', 'transformer.h.17.mlp.fc_in.bias', 'transformer.h.17.mlp.fc_out.weight', 'transformer.h.17.mlp.fc_out.bias', 'transformer.h.18.ln_1.weight', 'transformer.h.18.ln_1.bias', 'transformer.h.18.attn.k_proj.weight', 'transformer.h.18.attn.v_proj.weight', 'transformer.h.18.attn.q_proj.weight', 'transformer.h.18.attn.out_proj.weight', 'transformer.h.18.mlp.fc_in.weight', 'transformer.h.18.mlp.fc_in.bias', 'transformer.h.18.mlp.fc_out.weight', 'transformer.h.18.mlp.fc_out.bias', 'transformer.h.19.ln_1.weight', 'transformer.h.19.ln_1.bias', 'transformer.h.19.attn.k_proj.weight', 'transformer.h.19.attn.v_proj.weight', 'transformer.h.19.attn.q_proj.weight', 'transformer.h.19.attn.out_proj.weight', 'transformer.h.19.mlp.fc_in.weight', 'transformer.h.19.mlp.fc_in.bias', 'transformer.h.19.mlp.fc_out.weight', 'transformer.h.19.mlp.fc_out.bias', 'transformer.h.20.ln_1.weight', 'transformer.h.20.ln_1.bias', 'transformer.h.20.attn.k_proj.weight', 'transformer.h.20.attn.v_proj.weight', 'transformer.h.20.attn.q_proj.weight', 'transformer.h.20.attn.out_proj.weight', 'transformer.h.20.mlp.fc_in.weight', 'transformer.h.20.mlp.fc_in.bias', 'transformer.h.20.mlp.fc_out.weight', 'transformer.h.20.mlp.fc_out.bias', 'transformer.h.21.ln_1.weight', 'transformer.h.21.ln_1.bias', 'transformer.h.21.attn.k_proj.weight', 'transformer.h.21.attn.v_proj.weight', 'transformer.h.21.attn.q_proj.weight', 'transformer.h.21.attn.out_proj.weight', 'transformer.h.21.mlp.fc_in.weight', 'transformer.h.21.mlp.fc_in.bias', 'transformer.h.21.mlp.fc_out.weight', 'transformer.h.21.mlp.fc_out.bias', 'transformer.h.22.ln_1.weight', 'transformer.h.22.ln_1.bias', 'transformer.h.22.attn.k_proj.weight', 'transformer.h.22.attn.v_proj.weight', 'transformer.h.22.attn.q_proj.weight', 'transformer.h.22.attn.out_proj.weight', 'transformer.h.22.mlp.fc_in.weight', 'transformer.h.22.mlp.fc_in.bias', 'transformer.h.22.mlp.fc_out.weight', 'transformer.h.22.mlp.fc_out.bias', 'transformer.h.23.ln_1.weight', 'transformer.h.23.ln_1.bias', 'transformer.h.23.attn.k_proj.weight', 'transformer.h.23.attn.v_proj.weight', 'transformer.h.23.attn.q_proj.weight', 'transformer.h.23.attn.out_proj.weight', 'transformer.h.23.mlp.fc_in.weight', 'transformer.h.23.mlp.fc_in.bias', 'transformer.h.23.mlp.fc_out.weight', 'transformer.h.23.mlp.fc_out.bias', 'transformer.h.24.ln_1.weight', 'transformer.h.24.ln_1.bias', 'transformer.h.24.attn.k_proj.weight', 'transformer.h.24.attn.v_proj.weight', 'transformer.h.24.attn.q_proj.weight', 'transformer.h.24.attn.out_proj.weight', 'transformer.h.24.mlp.fc_in.weight', 'transformer.h.24.mlp.fc_in.bias', 'transformer.h.24.mlp.fc_out.weight', 'transformer.h.24.mlp.fc_out.bias', 'transformer.h.25.ln_1.weight', 'transformer.h.25.ln_1.bias', 'transformer.h.25.attn.k_proj.weight', 'transformer.h.25.attn.v_proj.weight', 'transformer.h.25.attn.q_proj.weight', 'transformer.h.25.attn.out_proj.weight', 'transformer.h.25.mlp.fc_in.weight', 'transformer.h.25.mlp.fc_in.bias', 'transformer.h.25.mlp.fc_out.weight', 'transformer.h.25.mlp.fc_out.bias', 'transformer.h.26.ln_1.weight', 'transformer.h.26.ln_1.bias', 'transformer.h.26.attn.k_proj.weight', 'transformer.h.26.attn.v_proj.weight', 'transformer.h.26.attn.q_proj.weight', 'transformer.h.26.attn.out_proj.weight', 'transformer.h.26.mlp.fc_in.weight', 'transformer.h.26.mlp.fc_in.bias', 'transformer.h.26.mlp.fc_out.weight', 'transformer.h.26.mlp.fc_out.bias', 'transformer.h.27.ln_1.weight', 'transformer.h.27.ln_1.bias', 'transformer.h.27.attn.k_proj.weight', 'transformer.h.27.attn.v_proj.weight', 'transformer.h.27.attn.q_proj.weight', 'transformer.h.27.attn.out_proj.weight', 'transformer.h.27.mlp.fc_in.weight', 'transformer.h.27.mlp.fc_in.bias', 'transformer.h.27.mlp.fc_out.weight', 'transformer.h.27.mlp.fc_out.bias', 'transformer.ln_f.weight', 'transformer.ln_f.bias', 'lm_head.weight', 'lm_head.bias']\n",
    "\n",
    "GPTJ_DICT = {}\n",
    "\n",
    "for i in range(28):\n",
    "    for layer in GPTJ_LAYERS:\n",
    "        if layer.startswith(f\"transformer.h.{i}.\"):\n",
    "            # Check if f\"Layer {i}\" is a key:\n",
    "            if f\"Layer {i}\" not in GPTJ_DICT.keys():\n",
    "                GPTJ_DICT[f\"Layer {i}\"] = []\n",
    "\n",
    "            GPTJ_DICT[f\"Layer {i}\"].append(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pop layer from GPTJ_LAYERS if it starts with transformer.h\n",
    "\n",
    "for i in reversed(range(len(GPTJ_LAYERS))):\n",
    "    layer = GPTJ_LAYERS[i]\n",
    "    if layer.startswith(\"transformer.h\"):\n",
    "        GPTJ_LAYERS.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transformer.wte.weight',\n",
       " 'transformer.ln_f.weight',\n",
       " 'transformer.ln_f.bias',\n",
       " 'lm_head.weight',\n",
       " 'lm_head.bias']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPTJ_LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPTJ_DICT[\"Embedding Layer\"] = [\"transformer.wte.weight\"]\n",
    "GPTJ_DICT[\"Final Layer Norm\"] = [\"transformer.ln_f.weight\", \"transformer.ln_f.bias\"]\n",
    "GPTJ_DICT[\"LM Head\"] = [\"lm_head.weight\", \"lm_head.bias\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tool_selection_dict = {}\n",
    "\n",
    "tokenized_tools = [[1,3,5,7,9],[1,3,5,6],[1,3,5],[2,3,5,7],[2]]\n",
    "\n",
    "OPEN_PARENTHESIS = \"PSYCH\"\n",
    "\n",
    "def tree_maker(tree, token, id, depth):\n",
    "    tokens = list(tree.keys())\n",
    "    if token not in tokens:\n",
    "        tree[token] = id\n",
    "    else:\n",
    "        if token == OPEN_PARENTHESIS:\n",
    "            print(f\"Warning: tool {tokenized_tools[id]} is already in the tree\")\n",
    "            return\n",
    "        # Check if instance of dictionary:\n",
    "        if not isinstance(tree[token], dict):\n",
    "            other_id = tree[token]\n",
    "            next_token = tokenized_tools[other_id][depth+1] if depth + 1 < len(tokenized_tools[other_id]) else OPEN_PARENTHESIS\n",
    "            tree[token] = {next_token: other_id}\n",
    "        next_token = tokenized_tools[id][depth+1] if depth + 1 < len(tokenized_tools[id]) else OPEN_PARENTHESIS\n",
    "        tree_maker(tree[token], next_token, id, depth + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: tool [1, 3, 5, 7, 9] is already in the tree\n",
      "Warning: tool [1, 3, 5, 6] is already in the tree\n",
      "Warning: tool [1, 3, 5] is already in the tree\n",
      "Warning: tool [2, 3, 5, 7] is already in the tree\n",
      "Warning: tool [2] is already in the tree\n"
     ]
    }
   ],
   "source": [
    "for i, tool in enumerate(tokenized_tools):\n",
    "    tree_maker(tool_selection_dict, tool[0], i, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {3: {5: {7: 0, 6: 1, 'PSYCH': 2}}}, 2: {3: 3, 'PSYCH': 4}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_selection_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Layer 0': ['transformer.h.0.ln_1.weight',\n",
       "  'transformer.h.0.ln_1.bias',\n",
       "  'transformer.h.0.attn.k_proj.weight',\n",
       "  'transformer.h.0.attn.v_proj.weight',\n",
       "  'transformer.h.0.attn.q_proj.weight',\n",
       "  'transformer.h.0.attn.out_proj.weight',\n",
       "  'transformer.h.0.mlp.fc_in.weight',\n",
       "  'transformer.h.0.mlp.fc_in.bias',\n",
       "  'transformer.h.0.mlp.fc_out.weight',\n",
       "  'transformer.h.0.mlp.fc_out.bias'],\n",
       " 'Layer 1': ['transformer.h.1.ln_1.weight',\n",
       "  'transformer.h.1.ln_1.bias',\n",
       "  'transformer.h.1.attn.k_proj.weight',\n",
       "  'transformer.h.1.attn.v_proj.weight',\n",
       "  'transformer.h.1.attn.q_proj.weight',\n",
       "  'transformer.h.1.attn.out_proj.weight',\n",
       "  'transformer.h.1.mlp.fc_in.weight',\n",
       "  'transformer.h.1.mlp.fc_in.bias',\n",
       "  'transformer.h.1.mlp.fc_out.weight',\n",
       "  'transformer.h.1.mlp.fc_out.bias'],\n",
       " 'Layer 2': ['transformer.h.2.ln_1.weight',\n",
       "  'transformer.h.2.ln_1.bias',\n",
       "  'transformer.h.2.attn.k_proj.weight',\n",
       "  'transformer.h.2.attn.v_proj.weight',\n",
       "  'transformer.h.2.attn.q_proj.weight',\n",
       "  'transformer.h.2.attn.out_proj.weight',\n",
       "  'transformer.h.2.mlp.fc_in.weight',\n",
       "  'transformer.h.2.mlp.fc_in.bias',\n",
       "  'transformer.h.2.mlp.fc_out.weight',\n",
       "  'transformer.h.2.mlp.fc_out.bias'],\n",
       " 'Layer 3': ['transformer.h.3.ln_1.weight',\n",
       "  'transformer.h.3.ln_1.bias',\n",
       "  'transformer.h.3.attn.k_proj.weight',\n",
       "  'transformer.h.3.attn.v_proj.weight',\n",
       "  'transformer.h.3.attn.q_proj.weight',\n",
       "  'transformer.h.3.attn.out_proj.weight',\n",
       "  'transformer.h.3.mlp.fc_in.weight',\n",
       "  'transformer.h.3.mlp.fc_in.bias',\n",
       "  'transformer.h.3.mlp.fc_out.weight',\n",
       "  'transformer.h.3.mlp.fc_out.bias'],\n",
       " 'Layer 4': ['transformer.h.4.ln_1.weight',\n",
       "  'transformer.h.4.ln_1.bias',\n",
       "  'transformer.h.4.attn.k_proj.weight',\n",
       "  'transformer.h.4.attn.v_proj.weight',\n",
       "  'transformer.h.4.attn.q_proj.weight',\n",
       "  'transformer.h.4.attn.out_proj.weight',\n",
       "  'transformer.h.4.mlp.fc_in.weight',\n",
       "  'transformer.h.4.mlp.fc_in.bias',\n",
       "  'transformer.h.4.mlp.fc_out.weight',\n",
       "  'transformer.h.4.mlp.fc_out.bias'],\n",
       " 'Layer 5': ['transformer.h.5.ln_1.weight',\n",
       "  'transformer.h.5.ln_1.bias',\n",
       "  'transformer.h.5.attn.k_proj.weight',\n",
       "  'transformer.h.5.attn.v_proj.weight',\n",
       "  'transformer.h.5.attn.q_proj.weight',\n",
       "  'transformer.h.5.attn.out_proj.weight',\n",
       "  'transformer.h.5.mlp.fc_in.weight',\n",
       "  'transformer.h.5.mlp.fc_in.bias',\n",
       "  'transformer.h.5.mlp.fc_out.weight',\n",
       "  'transformer.h.5.mlp.fc_out.bias'],\n",
       " 'Layer 6': ['transformer.h.6.ln_1.weight',\n",
       "  'transformer.h.6.ln_1.bias',\n",
       "  'transformer.h.6.attn.k_proj.weight',\n",
       "  'transformer.h.6.attn.v_proj.weight',\n",
       "  'transformer.h.6.attn.q_proj.weight',\n",
       "  'transformer.h.6.attn.out_proj.weight',\n",
       "  'transformer.h.6.mlp.fc_in.weight',\n",
       "  'transformer.h.6.mlp.fc_in.bias',\n",
       "  'transformer.h.6.mlp.fc_out.weight',\n",
       "  'transformer.h.6.mlp.fc_out.bias'],\n",
       " 'Layer 7': ['transformer.h.7.ln_1.weight',\n",
       "  'transformer.h.7.ln_1.bias',\n",
       "  'transformer.h.7.attn.k_proj.weight',\n",
       "  'transformer.h.7.attn.v_proj.weight',\n",
       "  'transformer.h.7.attn.q_proj.weight',\n",
       "  'transformer.h.7.attn.out_proj.weight',\n",
       "  'transformer.h.7.mlp.fc_in.weight',\n",
       "  'transformer.h.7.mlp.fc_in.bias',\n",
       "  'transformer.h.7.mlp.fc_out.weight',\n",
       "  'transformer.h.7.mlp.fc_out.bias'],\n",
       " 'Layer 8': ['transformer.h.8.ln_1.weight',\n",
       "  'transformer.h.8.ln_1.bias',\n",
       "  'transformer.h.8.attn.k_proj.weight',\n",
       "  'transformer.h.8.attn.v_proj.weight',\n",
       "  'transformer.h.8.attn.q_proj.weight',\n",
       "  'transformer.h.8.attn.out_proj.weight',\n",
       "  'transformer.h.8.mlp.fc_in.weight',\n",
       "  'transformer.h.8.mlp.fc_in.bias',\n",
       "  'transformer.h.8.mlp.fc_out.weight',\n",
       "  'transformer.h.8.mlp.fc_out.bias'],\n",
       " 'Layer 9': ['transformer.h.9.ln_1.weight',\n",
       "  'transformer.h.9.ln_1.bias',\n",
       "  'transformer.h.9.attn.k_proj.weight',\n",
       "  'transformer.h.9.attn.v_proj.weight',\n",
       "  'transformer.h.9.attn.q_proj.weight',\n",
       "  'transformer.h.9.attn.out_proj.weight',\n",
       "  'transformer.h.9.mlp.fc_in.weight',\n",
       "  'transformer.h.9.mlp.fc_in.bias',\n",
       "  'transformer.h.9.mlp.fc_out.weight',\n",
       "  'transformer.h.9.mlp.fc_out.bias'],\n",
       " 'Layer 10': ['transformer.h.10.ln_1.weight',\n",
       "  'transformer.h.10.ln_1.bias',\n",
       "  'transformer.h.10.attn.k_proj.weight',\n",
       "  'transformer.h.10.attn.v_proj.weight',\n",
       "  'transformer.h.10.attn.q_proj.weight',\n",
       "  'transformer.h.10.attn.out_proj.weight',\n",
       "  'transformer.h.10.mlp.fc_in.weight',\n",
       "  'transformer.h.10.mlp.fc_in.bias',\n",
       "  'transformer.h.10.mlp.fc_out.weight',\n",
       "  'transformer.h.10.mlp.fc_out.bias'],\n",
       " 'Layer 11': ['transformer.h.11.ln_1.weight',\n",
       "  'transformer.h.11.ln_1.bias',\n",
       "  'transformer.h.11.attn.k_proj.weight',\n",
       "  'transformer.h.11.attn.v_proj.weight',\n",
       "  'transformer.h.11.attn.q_proj.weight',\n",
       "  'transformer.h.11.attn.out_proj.weight',\n",
       "  'transformer.h.11.mlp.fc_in.weight',\n",
       "  'transformer.h.11.mlp.fc_in.bias',\n",
       "  'transformer.h.11.mlp.fc_out.weight',\n",
       "  'transformer.h.11.mlp.fc_out.bias'],\n",
       " 'Layer 12': ['transformer.h.12.ln_1.weight',\n",
       "  'transformer.h.12.ln_1.bias',\n",
       "  'transformer.h.12.attn.k_proj.weight',\n",
       "  'transformer.h.12.attn.v_proj.weight',\n",
       "  'transformer.h.12.attn.q_proj.weight',\n",
       "  'transformer.h.12.attn.out_proj.weight',\n",
       "  'transformer.h.12.mlp.fc_in.weight',\n",
       "  'transformer.h.12.mlp.fc_in.bias',\n",
       "  'transformer.h.12.mlp.fc_out.weight',\n",
       "  'transformer.h.12.mlp.fc_out.bias'],\n",
       " 'Layer 13': ['transformer.h.13.ln_1.weight',\n",
       "  'transformer.h.13.ln_1.bias',\n",
       "  'transformer.h.13.attn.k_proj.weight',\n",
       "  'transformer.h.13.attn.v_proj.weight',\n",
       "  'transformer.h.13.attn.q_proj.weight',\n",
       "  'transformer.h.13.attn.out_proj.weight',\n",
       "  'transformer.h.13.mlp.fc_in.weight',\n",
       "  'transformer.h.13.mlp.fc_in.bias',\n",
       "  'transformer.h.13.mlp.fc_out.weight',\n",
       "  'transformer.h.13.mlp.fc_out.bias'],\n",
       " 'Layer 14': ['transformer.h.14.ln_1.weight',\n",
       "  'transformer.h.14.ln_1.bias',\n",
       "  'transformer.h.14.attn.k_proj.weight',\n",
       "  'transformer.h.14.attn.v_proj.weight',\n",
       "  'transformer.h.14.attn.q_proj.weight',\n",
       "  'transformer.h.14.attn.out_proj.weight',\n",
       "  'transformer.h.14.mlp.fc_in.weight',\n",
       "  'transformer.h.14.mlp.fc_in.bias',\n",
       "  'transformer.h.14.mlp.fc_out.weight',\n",
       "  'transformer.h.14.mlp.fc_out.bias'],\n",
       " 'Layer 15': ['transformer.h.15.ln_1.weight',\n",
       "  'transformer.h.15.ln_1.bias',\n",
       "  'transformer.h.15.attn.k_proj.weight',\n",
       "  'transformer.h.15.attn.v_proj.weight',\n",
       "  'transformer.h.15.attn.q_proj.weight',\n",
       "  'transformer.h.15.attn.out_proj.weight',\n",
       "  'transformer.h.15.mlp.fc_in.weight',\n",
       "  'transformer.h.15.mlp.fc_in.bias',\n",
       "  'transformer.h.15.mlp.fc_out.weight',\n",
       "  'transformer.h.15.mlp.fc_out.bias'],\n",
       " 'Layer 16': ['transformer.h.16.ln_1.weight',\n",
       "  'transformer.h.16.ln_1.bias',\n",
       "  'transformer.h.16.attn.k_proj.weight',\n",
       "  'transformer.h.16.attn.v_proj.weight',\n",
       "  'transformer.h.16.attn.q_proj.weight',\n",
       "  'transformer.h.16.attn.out_proj.weight',\n",
       "  'transformer.h.16.mlp.fc_in.weight',\n",
       "  'transformer.h.16.mlp.fc_in.bias',\n",
       "  'transformer.h.16.mlp.fc_out.weight',\n",
       "  'transformer.h.16.mlp.fc_out.bias'],\n",
       " 'Layer 17': ['transformer.h.17.ln_1.weight',\n",
       "  'transformer.h.17.ln_1.bias',\n",
       "  'transformer.h.17.attn.k_proj.weight',\n",
       "  'transformer.h.17.attn.v_proj.weight',\n",
       "  'transformer.h.17.attn.q_proj.weight',\n",
       "  'transformer.h.17.attn.out_proj.weight',\n",
       "  'transformer.h.17.mlp.fc_in.weight',\n",
       "  'transformer.h.17.mlp.fc_in.bias',\n",
       "  'transformer.h.17.mlp.fc_out.weight',\n",
       "  'transformer.h.17.mlp.fc_out.bias'],\n",
       " 'Layer 18': ['transformer.h.18.ln_1.weight',\n",
       "  'transformer.h.18.ln_1.bias',\n",
       "  'transformer.h.18.attn.k_proj.weight',\n",
       "  'transformer.h.18.attn.v_proj.weight',\n",
       "  'transformer.h.18.attn.q_proj.weight',\n",
       "  'transformer.h.18.attn.out_proj.weight',\n",
       "  'transformer.h.18.mlp.fc_in.weight',\n",
       "  'transformer.h.18.mlp.fc_in.bias',\n",
       "  'transformer.h.18.mlp.fc_out.weight',\n",
       "  'transformer.h.18.mlp.fc_out.bias'],\n",
       " 'Layer 19': ['transformer.h.19.ln_1.weight',\n",
       "  'transformer.h.19.ln_1.bias',\n",
       "  'transformer.h.19.attn.k_proj.weight',\n",
       "  'transformer.h.19.attn.v_proj.weight',\n",
       "  'transformer.h.19.attn.q_proj.weight',\n",
       "  'transformer.h.19.attn.out_proj.weight',\n",
       "  'transformer.h.19.mlp.fc_in.weight',\n",
       "  'transformer.h.19.mlp.fc_in.bias',\n",
       "  'transformer.h.19.mlp.fc_out.weight',\n",
       "  'transformer.h.19.mlp.fc_out.bias'],\n",
       " 'Layer 20': ['transformer.h.20.ln_1.weight',\n",
       "  'transformer.h.20.ln_1.bias',\n",
       "  'transformer.h.20.attn.k_proj.weight',\n",
       "  'transformer.h.20.attn.v_proj.weight',\n",
       "  'transformer.h.20.attn.q_proj.weight',\n",
       "  'transformer.h.20.attn.out_proj.weight',\n",
       "  'transformer.h.20.mlp.fc_in.weight',\n",
       "  'transformer.h.20.mlp.fc_in.bias',\n",
       "  'transformer.h.20.mlp.fc_out.weight',\n",
       "  'transformer.h.20.mlp.fc_out.bias'],\n",
       " 'Layer 21': ['transformer.h.21.ln_1.weight',\n",
       "  'transformer.h.21.ln_1.bias',\n",
       "  'transformer.h.21.attn.k_proj.weight',\n",
       "  'transformer.h.21.attn.v_proj.weight',\n",
       "  'transformer.h.21.attn.q_proj.weight',\n",
       "  'transformer.h.21.attn.out_proj.weight',\n",
       "  'transformer.h.21.mlp.fc_in.weight',\n",
       "  'transformer.h.21.mlp.fc_in.bias',\n",
       "  'transformer.h.21.mlp.fc_out.weight',\n",
       "  'transformer.h.21.mlp.fc_out.bias'],\n",
       " 'Layer 22': ['transformer.h.22.ln_1.weight',\n",
       "  'transformer.h.22.ln_1.bias',\n",
       "  'transformer.h.22.attn.k_proj.weight',\n",
       "  'transformer.h.22.attn.v_proj.weight',\n",
       "  'transformer.h.22.attn.q_proj.weight',\n",
       "  'transformer.h.22.attn.out_proj.weight',\n",
       "  'transformer.h.22.mlp.fc_in.weight',\n",
       "  'transformer.h.22.mlp.fc_in.bias',\n",
       "  'transformer.h.22.mlp.fc_out.weight',\n",
       "  'transformer.h.22.mlp.fc_out.bias'],\n",
       " 'Layer 23': ['transformer.h.23.ln_1.weight',\n",
       "  'transformer.h.23.ln_1.bias',\n",
       "  'transformer.h.23.attn.k_proj.weight',\n",
       "  'transformer.h.23.attn.v_proj.weight',\n",
       "  'transformer.h.23.attn.q_proj.weight',\n",
       "  'transformer.h.23.attn.out_proj.weight',\n",
       "  'transformer.h.23.mlp.fc_in.weight',\n",
       "  'transformer.h.23.mlp.fc_in.bias',\n",
       "  'transformer.h.23.mlp.fc_out.weight',\n",
       "  'transformer.h.23.mlp.fc_out.bias'],\n",
       " 'Layer 24': ['transformer.h.24.ln_1.weight',\n",
       "  'transformer.h.24.ln_1.bias',\n",
       "  'transformer.h.24.attn.k_proj.weight',\n",
       "  'transformer.h.24.attn.v_proj.weight',\n",
       "  'transformer.h.24.attn.q_proj.weight',\n",
       "  'transformer.h.24.attn.out_proj.weight',\n",
       "  'transformer.h.24.mlp.fc_in.weight',\n",
       "  'transformer.h.24.mlp.fc_in.bias',\n",
       "  'transformer.h.24.mlp.fc_out.weight',\n",
       "  'transformer.h.24.mlp.fc_out.bias'],\n",
       " 'Layer 25': ['transformer.h.25.ln_1.weight',\n",
       "  'transformer.h.25.ln_1.bias',\n",
       "  'transformer.h.25.attn.k_proj.weight',\n",
       "  'transformer.h.25.attn.v_proj.weight',\n",
       "  'transformer.h.25.attn.q_proj.weight',\n",
       "  'transformer.h.25.attn.out_proj.weight',\n",
       "  'transformer.h.25.mlp.fc_in.weight',\n",
       "  'transformer.h.25.mlp.fc_in.bias',\n",
       "  'transformer.h.25.mlp.fc_out.weight',\n",
       "  'transformer.h.25.mlp.fc_out.bias'],\n",
       " 'Layer 26': ['transformer.h.26.ln_1.weight',\n",
       "  'transformer.h.26.ln_1.bias',\n",
       "  'transformer.h.26.attn.k_proj.weight',\n",
       "  'transformer.h.26.attn.v_proj.weight',\n",
       "  'transformer.h.26.attn.q_proj.weight',\n",
       "  'transformer.h.26.attn.out_proj.weight',\n",
       "  'transformer.h.26.mlp.fc_in.weight',\n",
       "  'transformer.h.26.mlp.fc_in.bias',\n",
       "  'transformer.h.26.mlp.fc_out.weight',\n",
       "  'transformer.h.26.mlp.fc_out.bias'],\n",
       " 'Layer 27': ['transformer.h.27.ln_1.weight',\n",
       "  'transformer.h.27.ln_1.bias',\n",
       "  'transformer.h.27.attn.k_proj.weight',\n",
       "  'transformer.h.27.attn.v_proj.weight',\n",
       "  'transformer.h.27.attn.q_proj.weight',\n",
       "  'transformer.h.27.attn.out_proj.weight',\n",
       "  'transformer.h.27.mlp.fc_in.weight',\n",
       "  'transformer.h.27.mlp.fc_in.bias',\n",
       "  'transformer.h.27.mlp.fc_out.weight',\n",
       "  'transformer.h.27.mlp.fc_out.bias'],\n",
       " 'Embedding Layer': ['transformer.wte.weight'],\n",
       " 'Final Layer Norm': ['transformer.ln_f.weight', 'transformer.ln_f.bias'],\n",
       " 'LM Head': ['lm_head.weight', 'lm_head.bias']}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPTJ_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: 4}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = {1:2, 3:4}\n",
    "\n",
    "del a[1]\n",
    "\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "use ``del sl[index]`` and ``sl.add(value)`` instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msortedcontainers\u001b[39;00m \u001b[39mimport\u001b[39;00m SortedKeyList\n\u001b[1;32m      3\u001b[0m a \u001b[39m=\u001b[39m SortedKeyList([[\u001b[39m1\u001b[39m,\u001b[39m4\u001b[39m],[\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m],[\u001b[39m3\u001b[39m,\u001b[39m2\u001b[39m],[\u001b[39m4\u001b[39m,\u001b[39m1\u001b[39m]], key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m1\u001b[39m])\n\u001b[0;32m----> 5\u001b[0m a[\u001b[39m0\u001b[39;49m] \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m/vol/bitbucket/jg2619/toolformer-luci/oldtoolvenv/lib/python3.10/site-packages/sortedcontainers/sortedlist.py:917\u001b[0m, in \u001b[0;36mSortedList.__setitem__\u001b[0;34m(self, index, value)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Raise not-implemented error.\u001b[39;00m\n\u001b[1;32m    909\u001b[0m \n\u001b[1;32m    910\u001b[0m \u001b[39m``sl.__setitem__(index, value)`` <==> ``sl[index] = value``\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    914\u001b[0m \n\u001b[1;32m    915\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    916\u001b[0m message \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39muse ``del sl[index]`` and ``sl.add(value)`` instead\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 917\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(message)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: use ``del sl[index]`` and ``sl.add(value)`` instead"
     ]
    }
   ],
   "source": [
    "from sortedcontainers import SortedKeyList\n",
    "\n",
    "a = SortedKeyList([[1,4],[2,3],[3,2],[4,1]], key=lambda x: x[1])\n",
    "\n",
    "a[0] = [1,1]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# Test llama tokenizer\n",
    "\n",
    "from transformers import LlamaTokenizer\n",
    "\n",
    "\n",
    "cache_dir = \"/vol/bitbucket/jg2619/augmenting_llms/augmented_data_pipeline/toolformer/cache\"\n",
    "cache_option = {\"cache_dir\": cache_dir} if cache_dir else {} \n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\",\n",
    "                                            token=\"\",\n",
    "                                            **cache_option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1723, 30121]\n",
      "[22172, 590, 1024, 338, 29897, 30121]\n",
      "[10309]\n",
      "[1723]\n",
      "[10309, 29897]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(tokenizer.encode(\")→\"))\n",
    "print(tokenizer.encode(\"hello my name is)→\"))\n",
    "\n",
    "\n",
    "print(tokenizer.encode(\"→\"))\n",
    "\n",
    "print(tokenizer.encode(\")\"))\n",
    "\n",
    "print(tokenizer.encode(\"→)\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TensorType\n",
    "import torch\n",
    "\n",
    "a = TensorType(\"pt\", type=torch.long)\n",
    "\n",
    "a\n",
    "tokenizer.encode(\"Hello, my dog is cute\", return_tensors=\"pt\").dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1,2,3],[4,5,6]])\n",
    "\n",
    "i = torch.tensor([0,1]).unsqueeze(1)\n",
    "\n",
    "b = torch.tensor([1,2,3,4,5,6])\n",
    "\n",
    "b.unsqueeze(1)[b-1]\n",
    "\n",
    "torch.isin(b.unsqueeze(1), torch.tensor([1,3]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oldtoolvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
