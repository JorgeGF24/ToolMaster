{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/jg2619/toolformer-luci/oldtoolvenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import traceback\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from beartype import beartype\n",
    "from beartype.typing import List, Callable, Union, Dict, Tuple\n",
    "\n",
    "# Import parent class of AutoTokenizer\n",
    "from transformers import LlamaTokenizer, AutoTokenizer, GPT2Tokenizer\n",
    "\n",
    "pad_sequence = partial(pad_sequence, batch_first=True)\n",
    "\n",
    "PAD_TOKEN = \"[PAD]\"\n",
    "PAD_ID = -100\n",
    "ARROW_TOKEN = 39310\n",
    "TOOL_TOKEN_ID = 50400\n",
    "END_API_TOKEN = 50401\n",
    "OPEN_PARENTHESIS = \"(\"\n",
    "OPEN_PARENTHESIS_ID = 7\n",
    "CLOSE_PARENTHESIS = 8\n",
    "\n",
    "LOGIT_DISPLACEMENT = 0 # This is for models where model at position i gives logits of prediction AFTER seeing i. For models that give logits of prediction BEFORE seeing i, this should be 1.\n",
    "\n",
    "\n",
    "def log(t, eps=1e-20): return t.clamp(min=eps).log()\n",
    "\n",
    "\n",
    "def gumbel_noise(t):\n",
    "    noise = torch.zeros_like(t).uniform_(0, 1)\n",
    "    return -log(-log(noise))\n",
    "\n",
    "\n",
    "def gumbel_sample(t, temperature=1., dim=-1, eps=1e-10):\n",
    "    # Returns flat vector\n",
    "    if temperature == 0:\n",
    "        return t.argmax(dim=dim)\n",
    "\n",
    "    return ((t / max(temperature, eps)) + gumbel_noise(t)).argmax(dim=dim)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@beartype\n",
    "class ToolMaster(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        *,\n",
    "        available_tools: List[Callable],\n",
    "        arg_parsers: List[Callable],\n",
    "        tool_explanation_prompts: Union[List[torch.Tensor], List[str]],\n",
    "        tool_names: List[str],\n",
    "        tool_short_desc: Dict,  # of the form \"tool_name\": \"tool_short_desc\"\n",
    "        tokenizer: Union[LlamaTokenizer, AutoTokenizer, GPT2Tokenizer],\n",
    "        debug_level: int = 0,\n",
    "        log_dir: str = \"/vol/bitbucket/jg2619/augmenting_llms/model_training/models/logs\",\n",
    "        export_tool_execution: bool = False,\n",
    "    ): \n",
    "        super().__init__()\n",
    "\n",
    "        global PAD_ID, PAD_TOKEN, OPEN_PARENTHESIS_ID, TOOL_TOKEN_ID, LOGIT_DISPLACEMENT\n",
    "        \n",
    "        self.model = model\n",
    "        self.encode = tokenizer.encode\n",
    "        self.decode = tokenizer.decode\n",
    "\n",
    "        PAD_ID = tokenizer.pad_token_id\n",
    "        PAD_TOKEN = tokenizer.pad_token\n",
    "\n",
    "        OPEN_PARENTHESIS_ID = tokenizer.encode(PAD_TOKEN + OPEN_PARENTHESIS)[1:]\n",
    "\n",
    "        tokenized_tools = [tokenizer.encode(tool_name) for tool_name in tool_names]\n",
    "        self.tokenized_tools = tokenized_tools\n",
    "        self.tool_names = tool_names\n",
    "\n",
    "        tool_name_desc = [        ]\n",
    "        for tool_name in tool_names:\n",
    "            name_desc = tool_name\n",
    "            if tool_name in tool_short_desc:\n",
    "                name_desc += \" (\" + tool_short_desc[tool_name] +\")\"\n",
    "            tool_name_desc.append(tool_name)\n",
    "\n",
    "        self.available_tools_prompt = \"You can use these tools to help you answer: \" + \", \".join(tool_names) + \".\\n\\n\"\n",
    "        self.tokenized_available_tools_prompt = tokenizer.encode(self.available_tools_prompt)\n",
    "\n",
    "        tool_selection_dict = {}\n",
    "        # This function creates a decision tree for the tool selection. The model chooses at each depth the token with the highest probability, until it reaches a tool id.\n",
    "        def tree_maker(tree, token, id, depth):\n",
    "            tokens = list(tree.keys())\n",
    "            if token not in tokens:\n",
    "                tree[token] = id\n",
    "            else:\n",
    "                if token == OPEN_PARENTHESIS_ID:\n",
    "                    print(f\"Warning: tool {tokenized_tools[id]} is already in the tree\")\n",
    "                    return\n",
    "                # Check if instance of dictionary:\n",
    "                if not isinstance(tree[token], dict):\n",
    "                    other_id = tree[token]\n",
    "                    next_token = tokenized_tools[other_id][depth+1] if depth + 1 < len(tokenized_tools[other_id]) else OPEN_PARENTHESIS_ID\n",
    "                    tree[token] = {next_token: other_id}\n",
    "                next_token = tokenized_tools[id][depth+1] if depth + 1 < len(tokenized_tools[id]) else OPEN_PARENTHESIS_ID\n",
    "                tree_maker(tree[token], next_token, id, depth + 1)\n",
    "\n",
    "        for i, tool in enumerate(tokenized_tools):\n",
    "            tree_maker(tool_selection_dict, tool[0], i, 0)\n",
    "\n",
    "\n",
    "        self.tool_selection_dict = tool_selection_dict\n",
    "        if isinstance(tool_explanation_prompts[0], str):\n",
    "            tool_explanation_prompts = [tokenizer.encode(prompt) for prompt in tool_explanation_prompts]\n",
    "        self.tool_explanation_prompts = tool_explanation_prompts\n",
    "        self.tools = available_tools\n",
    "        self.arg_parsers = arg_parsers\n",
    "        self.tokenized_tools = tokenized_tools\n",
    "\n",
    "        self.eos_token_id = tokenizer.eos_token_id\n",
    "        self.debug_level = debug_level\n",
    "        # Create log dir\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "        # count files in log dir\n",
    "        i = len(os.listdir(log_dir))\n",
    "        logging.basicConfig(filename=f'{log_dir}/{i}.log', level=logging.DEBUG if debug_level>0 else logging.INFO, format='%(asctime)s:  %(message)s', datefmt='%m/%d/%Y %I:%M:%S  ')\n",
    "        print(f\"Logging to {log_dir}/{i}.log\")\n",
    "\n",
    "        # Tokens with →\n",
    "        self.arg_gen_stoppers = []\n",
    "        for key, value in tokenizer.get_vocab().items():\n",
    "            if \"→\" in key:\n",
    "                self.arg_gen_stoppers.append(value)\n",
    "        self.arg_gen_stoppers = torch.tensor(self.arg_gen_stoppers).to(self.model.device)\n",
    "\n",
    "        self.export_tool_execution = export_tool_execution\n",
    "\n",
    "        # COPY PROMPT X TIMES\n",
    "        # self.tokenized_available_tools_prompt.unsqueeze(0).repeat(batch_size,1)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, \n",
    "                 primes: List[torch.Tensor], \n",
    "                 prompts: Union[List[torch.Tensor], torch.Tensor],\n",
    "                 tool_history: tuple[List[int],...] = None,\n",
    "                 arg_selection_mode: bool = False,  # Arg selection mode VS free generation augmented with tool selection\n",
    "                 batch_generated_count: torch.Tensor = None,\n",
    "                 max_includes_tool_usage: bool = True,\n",
    "                 max_new_tokens: int = 100,\n",
    "                 temperature: float = 0.1, \n",
    "                 stop_tokens: Union[List[int],int,torch.Tensor] = 198,):\n",
    "\n",
    "        device = self.model.device\n",
    "\n",
    "        if isinstance(prompts, torch.Tensor):\n",
    "            prompts = [prompts for _ in range(len(primes))]\n",
    "\n",
    "        assert primes[0].dim() == 1, \"Primes must be 1D tensor with the tokenized data\"\n",
    "        assert prompts[0].dim() == 1, \"Prompt must be 1D tensor with the tokenized prompt\"\n",
    "        \n",
    "        batch_size = len(primes)                                                        # BATCH SIZE\n",
    "        prompt_lens = torch.tensor([prompt.shape[0] for prompt in prompts]).to(device)  # LENGTHS OF PREPENDED PROMPTS\n",
    "        \n",
    "        # Position of where to start generating for each row\n",
    "        positions = torch.tensor([prime.shape[0] for prime in primes]).to(device).unsqueeze(1)\n",
    "        positions += prompt_lens.unsqueeze(1)\n",
    "        initial_positions = positions.clone()\n",
    "        if batch_generated_count is None:     # Count of tokens generated for each row\n",
    "            batch_generated_count = torch.zeros(batch_size, dtype=torch.int)\n",
    "        if tool_history is None:        # History of tools used for each row\n",
    "            tool_history = [[] for _ in range(batch_size)]\n",
    "        generated_content = [None for _ in range(batch_size)]\n",
    "        if not isinstance(stop_tokens, torch.Tensor):\n",
    "            stop_tokens = torch.tensor([stop_tokens]).to(device).long().view(-1)\n",
    "\n",
    "        batch_input = [torch.cat([prompt.to(device), prime.to(device)]) for prompt, prime in zip(prompts, primes)]\n",
    "        batch_lengths = torch.tensor([row.shape[0] for row in batch_input]).to(device)\n",
    "        batch_input = pad_sequence(batch_input, padding_value=PAD_ID)\n",
    "        padding_count = (batch_lengths + max_new_tokens - batch_generated_count).max().item()\n",
    "        batch_input = F.pad(batch_input, (0, padding_count,), value=PAD_ID)\n",
    "\n",
    "        \n",
    "        # Indexing tensor utils\n",
    "        loop_to_data_idx = torch.arange(batch_size).to(device)                 # Mapping from loop index to batch index\n",
    "        batch_indices = torch.arange(batch_size).to(device).unsqueeze(1)       # ARANGE THAT ADJUSTS TO THE LOOP BATCH SIZE AS SAMPLES FINISH\n",
    "        \n",
    "        # Tool selection utils\n",
    "        loop_selection_depth = torch.zeros(batch_size).int().to(device)        # Depth of the tool selection tree\n",
    "        loop_is_selecting_tools = torch.zeros(batch_size).bool().to(device)    # Indices where we are selecting a tool\n",
    "        initial_opts = torch.tensor(list(self.tool_selection_dict.keys()))     # Initial tool options\n",
    "        initial_opts = initial_opts.to(device).unsqueeze(1)\n",
    "        current_opts = [initial_opts for _ in range(batch_size)]               # Current tool options for row\n",
    "\n",
    "        count_done = 0\n",
    "        loop_i = batch_generated_count.min().item()    # i is min value of generated_count:\n",
    "        while loop_i < max_new_tokens and count_done < batch_size:\n",
    "            # MODEL FORWARD CALL. MAINTAINS SHAPE EVEN AFTER INDEXING\n",
    "            loop_last_logits= self.model(batch_input[loop_to_data_idx], use_cache=False).logits[batch_indices, positions[loop_to_data_idx] + LOGIT_DISPLACEMENT]\n",
    "\n",
    "            if arg_selection_mode:   # Tool usage not available\n",
    "                loop_last_logits[:, TOOL_TOKEN_ID] = 0\n",
    "\n",
    "            # Gumbel sample for rows not selecting a tool. Tool selection has different sampling procedure\n",
    "            sample_ids = loop_to_data_idx[~loop_is_selecting_tools]\n",
    "            loop_sampled = torch.ones(batch_indices.shape[0], 1).long().to(device)*-1\n",
    "            loop_sampled[~loop_is_selecting_tools] = gumbel_sample(loop_last_logits[~loop_is_selecting_tools], temperature=temperature)\n",
    "            batch_input[sample_ids.unsqueeze(1), positions[sample_ids]] = loop_sampled\n",
    "            batch_generated_count[sample_ids] += 1\n",
    "\n",
    "            # Sampling procedure for rows selecting a tool\n",
    "            if loop_is_selecting_tools.any():\n",
    "\n",
    "                for loop_i in reversed(loop_is_selecting_tools.nonzero().squeeze(1)):\n",
    "                    \n",
    "                    data_i = loop_to_data_idx[loop_i].item()\n",
    "                    # Tool names are composed of tokens. ie. [CAL] [CUL] [ATOR]. We call each token a syllable\n",
    "                    # Options for the next syllable. \n",
    "                    syllable_opts = torch.tensor(list(current_opts[loop_i].keys())).to(device)\n",
    "                    next_syllable_idx = loop_last_logits[loop_i,syllable_opts].argmax(dim=-1)\n",
    "                    next_syllable = syllable_opts[next_syllable_idx].item()\n",
    "                    batch_input[data_i, positions[loop_i]] = next_syllable\n",
    "                    loop_selection_depth[loop_i] += 1\n",
    "                    current_opts[data_i] = current_opts[data_i][next_syllable]\n",
    "\n",
    "                    # If current opts is a dict, there is a tie between possible tools. We need to keep selecting syllables.\n",
    "                    if not isinstance(current_opts[data_i], dict):   # ELSE: We've reached a tool id\n",
    "                        tool_id = current_opts[data_i]\n",
    "                        depth = loop_selection_depth[loop_i].item()+1   # Selection_depth = i means we've selected the ith syllable of tool name. Add 1 for indexing purposes\n",
    "                        tool_len = len(self.tokenized_tools[tool_id])\n",
    "                        batch_input[data_i, positions[data_i]-depth:positions[data_i]-depth+tool_len] = torch.tensor(self.tokenized_tools[tool_id]).to(device)\n",
    "                        batch_input[data_i, positions[data_i]-depth+tool_len] = OPEN_PARENTHESIS_ID\n",
    "                        batch_generated_count[data_i] += tool_len\n",
    "                        generated_content[data_i] = batch_input[data_i, initial_positions[data_i]:positions[data_i]]\n",
    "\n",
    "                        tool_history[data_i].append(tool_id)\n",
    "\n",
    "                        # Remove index i\n",
    "                        remove_index = torch.arange(loop_to_data_idx.shape[0]) != loop_i\n",
    "                        loop_is_selecting_tools = loop_is_selecting_tools[remove_index]\n",
    "                        loop_selection_depth = loop_selection_depth[remove_index]\n",
    "                        loop_to_data_idx = loop_to_data_idx[remove_index]\n",
    "                        loop_sampled = loop_sampled[remove_index]\n",
    "                        batch_indices[:-1]\n",
    "\n",
    "                        count_done += 1\n",
    "\n",
    "            # Check if any row wants to use a tool\n",
    "            just_sampled_tool = loop_sampled == TOOL_TOKEN_ID\n",
    "            if (just_sampled_tool).any():   # New rows selecting tools!\n",
    "                loop_is_selecting_tools[~loop_is_selecting_tools][just_sampled_tool] = True\n",
    "\n",
    "            # Rows that reached the max number of tokens, we finish the call\n",
    "            reached_limit = batch_generated_count[loop_to_data_idx] == max_new_tokens\n",
    "            # Sequence that reached the stop token\n",
    "            finished = torch.isin(loop_sampled.squeeze(1), stop_tokens) + reached_limit\n",
    "            if finished.any():\n",
    "                if not arg_selection_mode:\n",
    "                    # These rows are done generating. Mark them as finished\n",
    "                    positions[loop_to_data_idx[finished]] = -1\n",
    "\n",
    "                for finished_i in finished.nonzero().squeeze(1):\n",
    "                    data_i = loop_to_data_idx[finished_i].item()\n",
    "                    generated_content[data_i] = batch_input[data_i, initial_positions[data_i]:positions[data_i]]\n",
    "                    count_done += 1\n",
    "                    if reached_limit[finished_i]:\n",
    "                        logging.warn(f\"Stopping generation at row {data_i} that reached the generation limit\")\n",
    "                        logging.warn(f\"Data: {self.decode(batch_input[data_i])}\")\n",
    "                        if arg_selection_mode:\n",
    "                            # model failed to generate arguments.\n",
    "                            logging.warn(f\"Model failed to generate arguments for: \\ndata: {self.decode(batch_input[data_i])}\")\n",
    "                            logging.warn(f\"Data id {data_i}\")\n",
    "                            logging.warn(f\"Tool history: {tool_history[data_i]}\")\n",
    "                            positions[data_i] = -1    # This marks tool use error - rectifies use and resumes generation\n",
    "                            generated_content[data_i] = torch.tensor([]).to(device).long()\n",
    "\n",
    "                loop_to_data_idx = loop_to_data_idx[~finished]\n",
    "                loop_is_selecting_tools = loop_is_selecting_tools[~finished]\n",
    "                loop_selection_depth = loop_selection_depth[~finished]\n",
    "                batch_indices = batch_indices[:-finished.sum().item()]\n",
    "\n",
    "            loop_i += 1\n",
    "            positions[loop_to_data_idx[batch_indices.view(-1)]] += 1\n",
    "\n",
    "        # Return positions to their position in data\n",
    "        finished_rows = positions == -1   # Rows with special code -1 have finished due to reaching the generation limit or due to tool use error\n",
    "        positions[~finished_rows] -= prompt_lens[~finished_rows.squeeze(1)]\n",
    "        if not max_includes_tool_usage:\n",
    "            batch_generated_count[~finished_rows] -= torch.tensor([self.tokenized_tools[tool_history[i][-1]].shape[0] for i in range(batch_size)]).to(device)[~finished_rows] + 2 # +2 for open parenthesis and <TOOL> token\n",
    "\n",
    "        output = {\n",
    "            \"output_sentences\": [torch.cat((p.to(device), g)) for (p, g) in zip(primes, generated_content)],\n",
    "            \"positions\": positions,\n",
    "            \"batch_generated_count\": batch_generated_count,\n",
    "            \"tool_history\": tool_history,\n",
    "            \"sampled_args\": generated_content,\n",
    "        }\n",
    "        if not arg_selection_mode:\n",
    "            del output[\"sampled_args\"]\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    def forward(self, \n",
    "                sentences: List[str],):\n",
    "\n",
    "        # We receive a batch of texts. \n",
    "        logging.info(\"FORWARD TOOLMASTER\")\n",
    "        logging.info(f\"Received batch of {len(sentences)} sentences\")\n",
    "\n",
    "        device = self.model.device\n",
    "\n",
    "        # We tokenize the texts and store then in tuples with (tokenized_sentence, pos, count generation, tool_history)\n",
    "        pending_completion = [(self.encode(sentence), 0, []) for sentence in sentences]\n",
    "        finished_sentences = []\n",
    "\n",
    "        while len(pending_completion) > 0:\n",
    "\n",
    "            ####################################################\n",
    "            # FREE GENERATION MODE AUGMENTED WITH TOOL SELECTION\n",
    "            ####################################################\n",
    "\n",
    "            i = 0\n",
    "            batch_size = 11\n",
    "            pending_arg_sampling = []\n",
    "            pending_count = len(pending_completion)\n",
    "\n",
    "            while pending_count > 0:\n",
    "                logging.debug(f\"Processing batch {i+1}. Sentences processed: {len(pending_completion)-pending_count}/{len(pending_completion)}   ({(len(pending_completion)-pending_count)/len(pending_completion)*100:.2f}%))\")\n",
    "                sentence_batch = pending_completion[pending_count-batch_size:pending_count]\n",
    "\n",
    "                try:\n",
    "                    primes,  gen_count, tool_history = zip(*sentence_batch)\n",
    "                    output_dict = self.generate(primes = [torch.tensor(prime).to(device).long() for prime in primes],\n",
    "                                                prompts = torch.tensor(self.tokenized_available_tools_prompt).to(device).long(),\n",
    "                                                tool_history=tool_history,\n",
    "                                                batch_generated_count=torch.tensor(gen_count).to(device),\n",
    "                                                max_new_tokens = 100,\n",
    "                                                max_includes_tool_usage = True,\n",
    "                                                arg_selection_mode = False,\n",
    "                                                stop_tokens=self.eos_token_id)            \n",
    "                except torch.cuda.OutOfMemoryError as e: # type: ignore\n",
    "                    batch_size-=5\n",
    "                    sentence_batch = sentence_batch[5:]\n",
    "                    logging.info(f\"Out of memory error. Reducing batch size to {batch_size}\")\n",
    "                    continue\n",
    "                \n",
    "                pending_count -= batch_size      \n",
    "                finished_count = 0\n",
    "                tools_called = [0 for _ in range(len(self.tools))]\n",
    "                for sentence, status, gen_count, tool_history in zip(*output_dict.values()):\n",
    "                    if status == -1:\n",
    "                        finished_sentences.append((sentence.cpu(), tool_history))\n",
    "                        finished_count += 1\n",
    "                        for tool_id in tool_history:\n",
    "                            tools_called[tool_id] += 1\n",
    "                    else:\n",
    "                        pending_arg_sampling.append((sentence, gen_count, tool_history))\n",
    "\n",
    "                logging.info(f\"Batch {i+1} processed. Finished sentences: {finished_count}/{batch_size}, rest use tools.\")\n",
    "                logging.info(f\"Tools were called the following number of times:\")\n",
    "                for tool_name, tool_count in zip(self.tool_names, tools_called):\n",
    "                    logging.info(f\"{tool_name}: {tool_count}\")\n",
    "                i+=1\n",
    "\n",
    "\n",
    "            ####################################################\n",
    "            # ARGUMENT GENERATION MODE\n",
    "            ####################################################\n",
    "\n",
    "            batch_size = 11\n",
    "            pending_completion = []\n",
    "            pending_tool_execution = []\n",
    "            total_pending_args = len(pending_arg_sampling)\n",
    "            pending_count = total_pending_args\n",
    "\n",
    "            while pending_count > 0:\n",
    "                logging.debug(f\"Processing batch {i+1}. Sentences processed: {pending_count}/{total_pending_args}   ({pending_count/total_pending_args*100:.2f}%))\")\n",
    "                \n",
    "                sentence_batch = pending_arg_sampling[pending_count-batch_size:pending_count]\n",
    "                try:\n",
    "                    sentences, gen_count, tool_histories = zip(*sentence_batch)\n",
    "                    prompts = [self.tool_explanation_prompts[hist[-1]] for hist in tool_histories]\n",
    "                    output_dict = self.generate(primes = sentences,\n",
    "                                                prompts = torch.tensor(prompts).to(device).long(),\n",
    "                                                tool_history=tool_histories,\n",
    "                                                batch_generated_count=gen_count,\n",
    "                                                max_new_tokens = 100,\n",
    "                                                max_includes_tool_usage = True,\n",
    "                                                arg_selection_mode = True,\n",
    "                                                stop_tokens=self.arg_gen_stoppers)\n",
    "                except torch.cuda.OutOfMemoryError as e: # type: ignore\n",
    "                    batch_size-=5\n",
    "                    sentence_batch = sentence_batch[5:]\n",
    "                    logging.info(f\"Out of memory error. Reducing batch size to {batch_size}\")\n",
    "                    continue\n",
    "                \n",
    "                pending_count -= batch_size\n",
    "                finished_count = 0\n",
    "                for i, (sentence, status, gen_count, tool_history, sampled_args) in enumerate(zip(*output_dict.values())):\n",
    "                    if status == -1:\n",
    "                        pending_completion.append((sentence, gen_count, tool_history))\n",
    "                        finished_count += 1\n",
    "                    else:\n",
    "                        # TOOL SELECTION BABY\n",
    "                        pending_tool_execution.append((sentence, gen_count, tool_history, sampled_args))\n",
    "\n",
    "            ####################################################\n",
    "            # TOOL EXECUTION\n",
    "            ####################################################\n",
    "\n",
    "            if not self.export_tool_execution:\n",
    "                for sentence, gen_count, tool_history, sampled_args in pending_tool_execution:\n",
    "                    tool_id = tool_history[-1]\n",
    "                    try:\n",
    "                        args = self.arg_parsers[tool_id](self.decode(sampled_args))\n",
    "\n",
    "                        logging.info(f\"Executing tool {self.tool_names[tool_id]} with args {args}\")\n",
    "                        tool_output = self.tools[tool_id](*args)\n",
    "                        tool_output = self.encode(str(tool_output), return_tensors=\"pt\").to(device).long()\n",
    "\n",
    "                        sentence = torch.cat(sentence, tool_output.long())\n",
    "                    except Exception as e:\n",
    "                        logging.warn(f\"Error executing tool {self.tool_names[tool_id]} with args {args}\")\n",
    "                        # Print stack trace\n",
    "                        logging.warn(traceback.format_exc())\n",
    "                        logging.warn(f\"Error: {e}\")\n",
    "                        tool_output = \"Error executing tool\"\n",
    "\n",
    "                        # Remove bad call from sentence\n",
    "                        sentence = sentence[:-sampled_args.shape[0]]\n",
    "\n",
    "                    pending_completion.append((sentence, gen_count, tool_history))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if self.export_tool_execution:\n",
    "            return finished_sentences, pending_tool_execution\n",
    "        \n",
    "        return finished_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-28 02:17:14,380] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 768)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
    "\n",
    "\n",
    "\n",
    "cache_dir = \"/vol/bitbucket/jg2619/augmenting_llms/augmented_data_pipeline/toolformer/cache\"\n",
    "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\", cache_dir=cache_dir)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\", cache_dir=cache_dir)\n",
    "\n",
    "\n",
    "tokenizer.pad_token = \"!\"\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /vol/bitbucket/jg2619/augmenting_llms/model_training/models/logs/3.log\n",
      "You are a language model that can access external tools to aid in your responses. You can use the \"Calculator\" tool to perform calculations.\n",
      "The calls should help you get information required to complete the text. \n",
      "You can use the tool by writing \"[Calculator(expression)]\" where \"expression\" is the expression to be computed. \n",
      "Here are some examples of API calls:\n",
      "\n",
      "Example 1: The number in the next term is 18 + 12 x 3 = [Calculator(18 + 12 * 3)] 54.\n",
      "\n",
      "Example 2: The population is 658,893 people. This is 11.4% of the national average of [Calculator(658,893 / 11.4%)] 5,763,868 people.\n",
      "\n",
      "Example 3: A total of 252 qualifying matches were played, and 723 goals were scored (an average of [Calculator(723 / 252)] 2.87 per match). This is twenty goals more than the [Calculator(723 - 20)] 703 goals last year.\n",
      "\n",
      "Example 4: I went to Paris in 1994 and stayed there until 2011, so in total, it was [Calculator(2011 - 1994)] 17 years.\n",
      "\n",
      "Example 5: From this, we have 4 * 30 minutes = [Calculator(4 * 30)] 120 minutes.\n",
      "\n",
      "\n",
      "Your task now is to answer the following question. Do a step by step reasoning before writing \"Answer:\" and your answer.\n",
      "\n",
      "Question: Alex, Stan, and Adelwolfe are trying to catch them all, Pokemon that is.  Together they have caught 339 Pokemon.  Alex has caught 5 more than Stan, and Stan has caught 13 less than 4 times as many as Adelwolfe has caught. How many Pokemon has Stan caught?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_129456/1926225756.py:221: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n",
      "  logging.warn(f\"Stopping generation at row {data_i} that reached the generation limit\")\n",
      "/tmp/ipykernel_129456/1926225756.py:222: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n",
      "  logging.warn(f\"Data: {self.decode(batch_input[data_i])}\")\n"
     ]
    }
   ],
   "source": [
    "toolmaster = ToolMaster(model,\n",
    "                        available_tools = [lambda x: x, lambda x: x],\n",
    "                        arg_parsers = [lambda x: x, lambda x: x],\n",
    "                        tool_explanation_prompts = [\"Calculator\"],\n",
    "                        tool_names = [\"Calculator\"],\n",
    "                        tool_short_desc = {},\n",
    "                        tokenizer = tokenizer,\n",
    "                        debug_level = 1,\n",
    "                        log_dir = \"/vol/bitbucket/jg2619/augmenting_llms/model_training/models/logs\",\n",
    "                        export_tool_execution = False,\n",
    "                        )\n",
    "\n",
    "#input = torch.tensor([tokenizer.encode(x) for x in [\"This is a sentence\", \"This is another sentence\"]]).long()\n",
    "\n",
    "#print(input)\n",
    "#model(input[torch.tensor([[0],[1]])])\n",
    "\n",
    "prompt = \"\"\"You are a language model that can access external tools to aid in your responses. You can use the \"Calculator\" tool to perform calculations.\n",
    "The calls should help you get information required to complete the text. \n",
    "You can use the tool by writing \"[Calculator(expression)]\" where \"expression\" is the expression to be computed. \n",
    "Here are some examples of API calls:\n",
    "\n",
    "Example 1: The number in the next term is 18 + 12 x 3 = [Calculator(18 + 12 * 3)] 54.\n",
    "\n",
    "Example 2: The population is 658,893 people. This is 11.4% of the national average of [Calculator(658,893 / 11.4%)] 5,763,868 people.\n",
    "\n",
    "Example 3: A total of 252 qualifying matches were played, and 723 goals were scored (an average of [Calculator(723 / 252)] 2.87 per match). This is twenty goals more than the [Calculator(723 - 20)] 703 goals last year.\n",
    "\n",
    "Example 4: I went to Paris in 1994 and stayed there until 2011, so in total, it was [Calculator(2011 - 1994)] 17 years.\n",
    "\n",
    "Example 5: From this, we have 4 * 30 minutes = [Calculator(4 * 30)] 120 minutes.\n",
    "\n",
    "\n",
    "Your task now is to answer the following question. Do a step by step reasoning before writing \"Answer:\" and your answer.\n",
    "\n",
    "Question: Alex, Stan, and Adelwolfe are trying to catch them all, Pokemon that is.  Together they have caught 339 Pokemon.  Alex has caught 5 more than Stan, and Stan has caught 13 less than 4 times as many as Adelwolfe has caught. How many Pokemon has Stan caught?\n",
    "\n",
    "Your response:\"\"\"\n",
    "\n",
    "for result, history in toolmaster([prompt]):\n",
    "    print(tokenizer.decode(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  464,  3797,  3332,   319,   262],\n",
       "        [  464,  9875, 11687,   625,   262]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([tokenizer.encode(sentence) for sentence in [\"The cat sat on the\", \"The cow jumped over the\"]]).long()\n",
    "\n",
    "torch.tensor([[1639,  460,  779,  777, 4899,  284, 1037,  345, 3280,   25, 2891,   16,\n",
    "           11, 2891,   17,   13,  628, 1212,  318,  257, 6827],\n",
    "        [1639,  460,  779,  777, 4899,  284, 1037,  345, 3280,   25, 2891,   16,\n",
    "           11, 2891,   17,   13,  628, 1212,  318, 1194, 6827]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 50257])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' floor fence'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_inputs = torch.tensor([tokenizer.encode(sentence) for sentence in [\"The cat sat on the\", \"The cow jumped over the\"]]).long()\n",
    "\n",
    "logits = model(batch_inputs).logits\n",
    "print(logits.shape)\n",
    "\n",
    "tokenizer.decode(gumbel_sample(model(batch_inputs).logits[:,4], temperature=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oldtoolvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
